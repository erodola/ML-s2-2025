{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Congratulations on joining **TrueSound Research** — where cutting-edge machine learning meets the future of music. Our clients are the best in the industry, and now ***you*** are part of the team shaping what comes next.\n","\n","**Your Task for Today:**\n","\n","Build a music generator that creates one chunk of music at a time — each note, beat, or phrase predicting the next — in true autoregressive style.\n","\n","The world is listening. Time to create something unforgettable.\n","\n","***Let's make some noise.***"],"metadata":{"id":"v9GzZ46cdhEl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rk5Tb6LylWSz"},"outputs":[],"source":["import torch\n","import torchaudio\n","import matplotlib.pyplot as plt\n","from dataclasses import dataclass\n","import typing as T\n","import numpy as np\n","import io\n","import numpy as np\n","from scipy.io import wavfile\n","\n","!pip install pydub\n","import pydub"]},{"cell_type":"code","source":["# @title Utility functions (run this cell)\n","\n","@dataclass(frozen=True)\n","class SpectrogramParams:\n","\n","    # Whether the audio is stereo or mono\n","    stereo: bool = False\n","\n","    # FFT parameters\n","    sample_rate: int = 44100\n","    step_size_ms: int = 10\n","    window_duration_ms: int = 100\n","    padded_duration_ms: int = 400\n","\n","    # Mel scale parameters\n","    num_frequencies: int = 512\n","    min_frequency: int = 0\n","    max_frequency: int = 10000  # TODO (Ema): probably better to use 20 to 20,000\n","    mel_scale_norm: T.Optional[str] = None\n","    mel_scale_type: str = \"htk\"\n","\n","    # Griffin Lim parameters\n","    num_griffin_lim_iters: int = 32\n","\n","    # Image parameterization\n","    power_for_image: float = 0.25\n","\n","    @property\n","    def n_fft(self) -> int:\n","        \"\"\"\n","        The number of samples in each STFT window, with padding.\n","        \"\"\"\n","        return int(self.padded_duration_ms / 1000.0 * self.sample_rate)\n","\n","    @property\n","    def win_length(self) -> int:\n","        \"\"\"\n","        The number of samples in each STFT window.\n","        \"\"\"\n","        return int(self.window_duration_ms / 1000.0 * self.sample_rate)\n","\n","    @property\n","    def hop_length(self) -> int:\n","        \"\"\"\n","        The number of samples between each STFT window.\n","        \"\"\"\n","        return int(self.step_size_ms / 1000.0 * self.sample_rate)\n","\n","\n","def audio_from_waveform(\n","    samples: np.ndarray, sample_rate: int, normalize: bool = False\n",") -> pydub.AudioSegment:\n","    \"\"\"\n","    Convert a numpy array of samples of a waveform to an audio segment.\n","\n","    Args:\n","        samples: (channels, samples) array\n","    \"\"\"\n","    # Normalize volume to fit in int16\n","    if normalize:\n","        samples *= np.iinfo(np.int16).max / np.max(np.abs(samples))\n","\n","    # Transpose and convert to int16\n","    samples = samples.transpose(1, 0)\n","    samples = samples.astype(np.int16)\n","\n","    # Write to the bytes of a WAV file\n","    wav_bytes = io.BytesIO()\n","    wavfile.write(wav_bytes, sample_rate, samples)\n","    wav_bytes.seek(0)\n","\n","    # Read into pydub\n","    return pydub.AudioSegment.from_wav(wav_bytes)\n","\n","def apply_filters(segment: pydub.AudioSegment, compression: bool = False) -> pydub.AudioSegment:\n","    \"\"\"\n","    Apply post-processing filters to the audio segment to compress it and\n","    keep at a -10 dBFS level.\n","    \"\"\"\n","\n","    if compression:\n","        segment = pydub.effects.normalize(\n","            segment,\n","            headroom=0.1,\n","        )\n","\n","        segment = segment.apply_gain(-10 - segment.dBFS)\n","\n","        segment = pydub.effects.compress_dynamic_range(\n","            segment,\n","            threshold=-20.0,\n","            ratio=4.0,\n","            attack=5.0,\n","            release=50.0,\n","        )\n","\n","    desired_db = -12\n","    segment = segment.apply_gain(desired_db - segment.dBFS)\n","\n","    segment = pydub.effects.normalize(\n","        segment,\n","        headroom=0.1,\n","    )\n","\n","    return segment\n","\n","\n","class SpectrogramConverter:\n","    \"\"\"\n","    Convert between audio segments and spectrogram tensors using torchaudio.\n","\n","    In this class a \"spectrogram\" is defined as a (batch, time, frequency) tensor with float values\n","    that represent the amplitude of the frequency at that time bucket (in the frequency domain).\n","    Frequencies are given in the perceptul Mel scale defined by the params. A more specific term\n","    used in some functions is \"mel amplitudes\".\n","\n","    The spectrogram computed from `spectrogram_from_audio` is complex valued, but it only\n","    returns the amplitude, because the phase is chaotic and hard to learn. The function\n","    `audio_from_spectrogram` is an approximate inverse of `spectrogram_from_audio`, which\n","    approximates the phase information using the Griffin-Lim algorithm.\n","\n","    Each channel in the audio is treated independently, and the spectrogram has a batch dimension\n","    equal to the number of channels in the input audio segment.\n","\n","    Both the Griffin Lim algorithm and the Mel scaling process are lossy.\n","\n","    For more information, see https://pytorch.org/audio/stable/transforms.html\n","    \"\"\"\n","\n","    def __init__(self, params: SpectrogramParams, device: str = \"cuda\"):\n","        self.p = params\n","\n","        self.device = device\n","\n","        if device.lower().startswith(\"mps\"):\n","            warnings.warn(\n","                \"WARNING: MPS does not support audio operations, falling back to CPU for them\",\n","                stacklevel=2,\n","            )\n","            self.device = \"cpu\"\n","\n","        # https://pytorch.org/audio/stable/generated/torchaudio.transforms.Spectrogram.html\n","        self.spectrogram_func = torchaudio.transforms.Spectrogram(\n","            n_fft=params.n_fft,\n","            hop_length=params.hop_length,\n","            win_length=params.win_length,\n","            pad=0,\n","            window_fn=torch.hann_window,\n","            power=None,\n","            normalized=False,\n","            wkwargs=None,\n","            center=True,\n","            pad_mode=\"reflect\",\n","            onesided=True,\n","        ).to(self.device)\n","\n","        # https://pytorch.org/audio/stable/generated/torchaudio.transforms.GriffinLim.html\n","        self.inverse_spectrogram_func = torchaudio.transforms.GriffinLim(\n","            n_fft=params.n_fft,\n","            n_iter=params.num_griffin_lim_iters,\n","            win_length=params.win_length,\n","            hop_length=params.hop_length,\n","            window_fn=torch.hann_window,\n","            power=1.0,\n","            wkwargs=None,\n","            momentum=0.99,\n","            length=None,\n","            rand_init=True,\n","        ).to(self.device)\n","\n","        # https://pytorch.org/audio/stable/generated/torchaudio.transforms.MelScale.html\n","        self.mel_scaler = torchaudio.transforms.MelScale(\n","            n_mels=params.num_frequencies,\n","            sample_rate=params.sample_rate,\n","            f_min=params.min_frequency,\n","            f_max=params.max_frequency,\n","            n_stft=params.n_fft // 2 + 1,\n","            norm=params.mel_scale_norm,\n","            mel_scale=params.mel_scale_type,\n","        ).to(self.device)\n","\n","        # https://pytorch.org/audio/stable/generated/torchaudio.transforms.InverseMelScale.html\n","        self.inverse_mel_scaler = torchaudio.transforms.InverseMelScale(\n","            n_stft=params.n_fft // 2 + 1,\n","            n_mels=params.num_frequencies,\n","            sample_rate=params.sample_rate,\n","            f_min=params.min_frequency,\n","            f_max=params.max_frequency,\n","            norm=params.mel_scale_norm,\n","            mel_scale=params.mel_scale_type,\n","        ).to(self.device)\n","\n","    def spectrogram_from_audio(\n","        self,\n","        audio: pydub.AudioSegment,\n","    ) -> np.ndarray:\n","        \"\"\"\n","        Compute a spectrogram from an audio segment.\n","\n","        Args:\n","            audio: Audio segment which must match the sample rate of the params\n","\n","        Returns:\n","            spectrogram: (channel, frequency, time)\n","        \"\"\"\n","        assert int(audio.frame_rate) == self.p.sample_rate, \"Audio sample rate must match params\"\n","\n","        # Get the samples as a numpy array in (batch, samples) shape\n","        waveform = np.array([c.get_array_of_samples() for c in audio.split_to_mono()])\n","\n","        # Convert to floats if necessary\n","        if waveform.dtype != np.float32:\n","            waveform = waveform.astype(np.float32)\n","\n","        waveform_tensor = torch.from_numpy(waveform).to(self.device)\n","        amplitudes_mel = self.mel_amplitudes_from_waveform(waveform_tensor)\n","\n","        spectrogram = amplitudes_mel.cpu().numpy()\n","\n","        return spectrogram[0] if spectrogram.shape[0] == 1 else spectrogram\n","\n","    def audio_from_spectrogram(\n","        self,\n","        spectrogram: np.ndarray,\n","        do_apply_filters: bool = True,\n","    ) -> pydub.AudioSegment:\n","        \"\"\"\n","        Reconstruct an audio segment from a spectrogram.\n","\n","        Args:\n","            spectrogram: (batch, frequency, time)\n","            apply_filters: Post-process with normalization and compression\n","\n","        Returns:\n","            audio: Audio segment with channels equal to the batch dimension\n","        \"\"\"\n","\n","        print(\"Converting... \", end='', flush=True)\n","\n","        # Move to device\n","        amplitudes_mel = torch.from_numpy(spectrogram).to(self.device)\n","\n","        # Reconstruct the waveform\n","        waveform = self.waveform_from_mel_amplitudes(amplitudes_mel)\n","\n","        # Convert to audio segment\n","        segment = audio_from_waveform(\n","            samples=waveform.cpu().numpy(),\n","            sample_rate=self.p.sample_rate,\n","            # Normalize the waveform to the range [-1, 1]\n","            normalize=True,\n","        )\n","\n","        # Optionally apply post-processing filters\n","        if do_apply_filters:\n","            segment = apply_filters(\n","                segment,\n","                compression=False,\n","            )\n","\n","        print(\"done.\")\n","\n","        return segment\n","\n","    def mel_amplitudes_from_waveform(\n","        self,\n","        waveform: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Torch-only function to compute Mel-scale amplitudes from a waveform.\n","\n","        Args:\n","            waveform: (batch, samples)\n","\n","        Returns:\n","            amplitudes_mel: (batch, frequency, time)\n","        \"\"\"\n","        # Compute the complex-valued spectrogram\n","        spectrogram_complex = self.spectrogram_func(waveform)\n","\n","        # Take the magnitude\n","        amplitudes = torch.abs(spectrogram_complex)\n","\n","        # Convert to mel scale\n","        return self.mel_scaler(amplitudes)\n","\n","    def waveform_from_mel_amplitudes(\n","        self,\n","        amplitudes_mel: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Torch-only function to approximately reconstruct a waveform from Mel-scale amplitudes.\n","\n","        Args:\n","            amplitudes_mel: (batch, frequency, time)\n","\n","        Returns:\n","            waveform: (batch, samples)\n","        \"\"\"\n","        # Convert from mel scale to linear\n","        amplitudes_linear = self.inverse_mel_scaler(amplitudes_mel)\n","\n","        # Run the approximate algorithm to compute the phase and recover the waveform\n","        return self.inverse_spectrogram_func(amplitudes_linear)"],"metadata":{"id":"tIi_YvdqqS-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"70nBTneewTCq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training data\n","\n","For this task, your training data is a collection of music scraped from the internet, in particular from [The Mod Archive](https://modarchive.org/) and [Amiga Music Preservation](https://amp.dascene.net/). This is old music from the days of the [Demoscene](https://en.wikipedia.org/wiki/Demoscene), and importantly, it's free!\n","\n","We have scraped the training data for you in mp3 format. **START DOWNLOADING NOW** in the interest of time!\n","\n","Get it from one of these mirrors (1.8GB): [[1]](https://drive.google.com/file/d/1HZs6l4eSXNqDUAd-SrvQMZYr-VwYVO9z/view?usp=sharing) [[2]](https://drive.google.com/file/d/1JRZE0fYmhm6Rs6O2GK87Ft0yLgH9o9sq/view?usp=sharing) [[3]](https://drive.google.com/file/d/17yp8E1N1PLw2bQ0Jsr3l3IHKbWumesBc/view?usp=sharing) [[4]](https://drive.google.com/file/d/1tTUX0C0awucasjj3vbRvzs6O4_tktLLE/view?usp=sharing) [[5]](https://drive.google.com/file/d/1D_cDR9CCfHYeg-UTFaUdFnoapw3xcUQ1/view?usp=sharing) [[6]](https://drive.google.com/file/d/1C0PoTZVL0i2LFk_b8pTbsQOUi2Jg6Xl9/view?usp=sharing) [[7]](https://drive.google.com/file/d/1-MgDBK_1-EtPAuaKkyTZfGW3efn-T4QO/view?usp=sharing)\n","\n","Please communicate with other groups in order to use one link per group to avoid bandwidth limitations from Google.\n","\n","Once you have downloaded the zip file, unzip it into the \"mp3\" folder and upload this new folder [on the root of your Google Drive](https://drive.google.com/drive/u/0/my-drive)\n","\n","To start experimenting immediately, [here](https://drive.google.com/file/d/1DZRyHKHWaIl0Qh_ahce9mcyEeLAKId7k/view?usp=sharing)'s a single test track (~2MB).\n","\n","Once the dataset is downloaded, mount your Google Drive (Colab sidebar → Files → Mount Drive) and test if it works:"],"metadata":{"id":"sPXOcacSe6xC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# replace with your own path\n","audio = pydub.AudioSegment.from_mp3(\"./drive/MyDrive/mp3/0istein Eide - Funkanette.mp3\")\n","\n","audio"],"metadata":{"id":"rmujrGN6hin8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data representation\n","\n","Once you have loaded a track, you can access its sequence of audio samples as follows:"],"metadata":{"id":"LCGk3GRsh8vX"}},{"cell_type":"code","source":["# just cropping 5 seconds of music\n","chunk = audio[20_000:25_000]\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(chunk.get_array_of_samples())\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"0NR1OfJTiLi5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's play it:"],"metadata":{"id":"dAfkG5X7j0Fj"}},{"cell_type":"code","source":["chunk"],"metadata":{"id":"DNYehGKQjzr0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can work with this representation, and treat it as a sequence of symbols just like you did with text in your name generator during yesterday's interview.\n","\n","Alternatively, you can work with **spectrograms**."],"metadata":{"id":"OaFpHqWginfC"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","params = SpectrogramParams(\n","            sample_rate=44100,\n","            stereo=False,\n","            step_size_ms=10,\n","            min_frequency=20,\n","            max_frequency=20000,\n","            num_frequencies=512,\n","        )\n","\n","sc = SpectrogramConverter(params=params, device=device)"],"metadata":{"id":"Oe_tLwJnokiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, let's compute a spectrogram for the five seconds of music from above:\n","\n","**WARNING:** Don't compute spectrograms for entire songs, it's slow and takes up a lot of RAM!"],"metadata":{"id":"0EZ4SgjajjIh"}},{"cell_type":"code","source":["chunk = audio[20_000:25_000]\n","spectrogram = sc.spectrogram_from_audio(chunk)\n","\n","plt.figure(figsize=(7,4))\n","plt.imshow(spectrogram, origin='lower', aspect='auto', interpolation=\"nearest\")\n","plt.ylabel('Freq. bin')\n","plt.xlabel('Time (ms)')\n","plt.title('Spectrogram', fontsize=10)\n","plt.show()"],"metadata":{"id":"C0qwRyEKjddS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The spectrogram is a 2D (time-frequency) representation for a piece of music.\n","\n","With this representation, you could think of your music as a sequence of vertical spectrogram slices, of a few milliseconds each.\n","\n","Therefore, you could generate spectrogram slices, put them together, and then convert back to the audio domain!\n","\n","Here's some code for converting spectrograms back to audio:"],"metadata":{"id":"XRRxHSPMkZQu"}},{"cell_type":"code","source":["# this is slow!\n","chunk_recon = sc.audio_from_spectrogram(spectrogram[None, :], do_apply_filters=False)\n","\n","chunk_recon"],"metadata":{"id":"WbmHTmuUjPlF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that the reconstruction is not perfect, due to quantization parameters in `params`.\n","\n","We don't care too much about sound quality though.\n","\n","We care about **training a working music generator!** That's why you were hired in the first place."],"metadata":{"id":"Ec6kyCHLlbx0"}},{"cell_type":"markdown","source":["### Training a music generator\n","\n","On to you: show us what you can do!"],"metadata":{"id":"jV3EV9nPmUpX"}},{"cell_type":"code","source":["# here"],"metadata":{"id":"ULVvyE8CmgYs"},"execution_count":null,"outputs":[]}]}