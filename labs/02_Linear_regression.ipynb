{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C5Ct9yoZKYa"
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "# Tutorial 2: Linear and polynomial regression\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Linear regression, with linear and polynomial features\n",
    "- Loss landscapes, L2 (Tikhonov) regularization\n",
    "- The scikit-learn library\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Prof. Emanuele Rodolà\n",
    "- Based in part on original material by Dr. Antonio Norelli and Dr. Luca Moschella\n",
    "\n",
    "Course:\n",
    "\n",
    "Lectures and notebooks at https://erodola.github.io/ML-s2-2025/\n",
    "\n",
    "#### Instructions\n",
    "We encourage you to form small groups of 2-3 people to read and discuss the notebooks together.\n",
    "\n",
    "Run the code and play with it! It is very easy to edit the code locally and make small experiments. Try whatever comes to your mind, this is the best way to learn! Python notebooks are designed to be used in this way, that's why we chose them for the DLAI lab sessions.\n",
    "\n",
    "There will be some exercises, try to do them by yourself, and when everyone in your group has finished, compare the solutions with each other.\n",
    "\n",
    "When something is not clear or you have a question, raise your hand and we will come to you, or directly approach us.\n",
    "\n",
    "Remember that parts denoted by the open book 📖 are _optional_.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHLO4Z-T_yxB"
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHKiAWxOGLb1"
   },
   "source": [
    "In this notebook we will extensively use **Plotly**, a powerful plotting library (there are many [visualization libraries](https://pyviz.org/overviews/index.html) for Python).\n",
    "Effectively communicating your findings through plots and visualizations is an essential part of any scientific or engineering project.\n",
    "Plotly is a modern library that makes interactive plots very easy to produce, and is very popular also outside of the ML community.\n",
    "\n",
    "[Here](https://plot.ly/python/) you can find its documentation.\n",
    "\n",
    "Remember: Don't take our suggestions as a dogma. If you prefer other plotting libraries, use them! In fact, we will be using different ones in this notebook to give you a gist of what's out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRePt-K1_yw9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcNRBDwVUlkl"
   },
   "source": [
    "### A note on reproducibility\n",
    "\n",
    "In our ML experiments, we will frequently encounter _randomness_. For example, whenever we want to generate some quick-and-dirty data to test our algorithms on the fly, we will just create some random matrices and points. In addition, many ML algorithms actually _rely on randomness_ due to the several beneficial properties it brings along! **Random forests** and **stochastic gradient descent** are two prominent examples.\n",
    "\n",
    "Here's an idea: for our tests, we want to ensure that we always generate \"the same randomness\", so that anyone who will use our code will be able to reproduce exactly our results.\n",
    "\n",
    "We do this by providing a hand-picked, fixed **seed** to the random number generator (RNG). The specific value of the feed is not important: what matters is that it's constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tGN_bJOcfd3"
   },
   "outputs": [],
   "source": [
    "# Reproducibility stuff\n",
    "import random\n",
    "np.random.seed(42)  # initialize NumPy's RNG\n",
    "random.seed(0)      # initialize Python's built-in RNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPHftSvUaIhe"
   },
   "source": [
    "## Linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCRBI35LWCvt"
   },
   "source": [
    "Today we'll work with **linear models**. Don't underestimate their power! Linear models are simple, but they are at the heart of several effective pipelines. They will provide solid baselines for our experiments at the very least, and are a useful tool to study more advanced topics in modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PAIT1NjFUhk"
   },
   "source": [
    "### The regression problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8htU_ACVGQY5"
   },
   "source": [
    "#### The general definition\n",
    "We observe a phenomenon $\\mathcal{P}$ where a variable of interest $t$ depends on another variable $x$.\n",
    "\n",
    "**Problem**: Given a set of $N$ paired observations $(x, t)$, $\\mathbf{D}=\\{(x_1, t_1),...,(x_N, t_N)\\}$, and a new *input* instance $x_i$, find the corresponding *target* $t_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8kq5mSFOHul"
   },
   "source": [
    "#### Our workspace\n",
    "\n",
    "In our workspace, the underlying law governing our phenomenon $\\mathcal{P}$ links $t$ to a $x \\in [0,1]$ through a $\\sin$ function.\n",
    "For our purposes, we are going to synthetically generate a set of observations --our *dataset*-- following this rule:\n",
    "\n",
    "$$t=\\sin(2\\pi x) + 0.1\\epsilon \\;\\;\\;\\;\\;\\;\\;\\;\\; \\epsilon \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "where $\\sim \\mathcal{N}(0,1)$ means \"*sampled from a gaussian distribution with $\\mu=0$ and $\\sigma=1$*\".\n",
    "\n",
    "In this way, we are simulating the measurement process of any phenomenon: there is a global and general regularity (that we wish to discover) but all observations are plagued by uncertainty, here modeled with gaussian noise. Uncertainty usually comes from the finite precision of our measuring instrument, but may also be due to sources of variability we do not care about. Sometimes the problem we are tackling is intrinsically stochastic --such as measurements in quantum mechanics or idealized scenarios like the multi-armed bandit-- and we want to figure out just the non-stochastic part of the underlying rule.\n",
    "\n",
    "This is our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJyjckvQE-aZ"
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(10)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2737,
     "status": "ok",
     "timestamp": 1710844037504,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "IMJNXRXZGJS0",
    "outputId": "ccd3ec12-1b79-4ba3-e7ee-2e7a35dde0b5"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=x, y=t)\n",
    "fig.update_layout(width=600, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN_Ao9LRMr7s"
   },
   "source": [
    "It seems we have very little to work with! Just a handful of noisy points.\n",
    "\n",
    "How is it possible to devise a rule $f$ that is valid in general --i.e. for all the *infinite* points $x_i$ between 0 and 1 shall occur $t_i=f(x_i)$-- from such a *finite* set of observations?\n",
    "\n",
    "If we collect just 10 points and each of them agrees with a precise rule, why does that give us *any grounds* --even probabilistic grounds-- to expect the 11[$^{st}$](https://translate.google.com/translate?sl=it&tl=en&u=https://it.wikipedia.org/wiki/Tacchino_induttivista) point to follow the same rule?\n",
    "\n",
    "This is the essence of the so called [*Problem of induction*](https://ar5iv.labs.arxiv.org/html/1108.1791#S7) raised by David Hume more than two centuries ago. Still today this problem is at the core of the philosophical debate on the nature of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhRdUyWpwX-v"
   },
   "source": [
    "### Where to restart from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P5vTHieHC2R"
   },
   "source": [
    "We will take a very pragmatic point of view. Even if induction cannot be justified in general, we note that \"often it seems to work\". Therefore we proceed with our quest to find $f$.\n",
    "\n",
    "First of all, we observe that the quality of our guess $f$ depends crucially on how much information about the phenomenon $\\mathcal{P}$ we are given in advance, i.e. what **priors** do we have.\n",
    "\n",
    "\n",
    "\n",
    "**EXERCISE**: Suppose you have to guess this general rule $f$ to make predictions for new $x_i \\in [0,1]$. In which of the following scenarios would you like to find yourself to make the most accurate predictions?\n",
    "\n",
    "Write down a ranking from best to worst scenario, and then compare your ranking with those of your neighbors.\n",
    "\n",
    "For each scenario below, we give you a description of the available priors together with a plot.\n",
    "\n",
    "\n",
    "> **Scenario A**\n",
    ">\n",
    "> *priors:*\n",
    "> - 20 observations $(x_i,t_i)$ in the interval $[0,1]$ affected by a gaussian noise $0.1*\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "> **Scenario B**\n",
    ">\n",
    "> *priors:*\n",
    "> - 20 observations $(x_i,t_i)$ in the interval $[0,1]$ affected by a gaussian noise $0.5*\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "> **Scenario C**\n",
    ">\n",
    "> *priors:*\n",
    "> - 40 observations $(x_i,t_i)$ in the interval $[0,1]$ affected by a gaussian noise $0.1*\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "> **Scenario D**\n",
    ">\n",
    "> *priors:*\n",
    "> - 100 observations $(x_i,t_i)$ in the interval $[0,0.3]$ affected by a gaussian noise $0.1*\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "> **Scenario E**\n",
    ">\n",
    "> *priors:*\n",
    "> - 10 observations $(x_i,t_i)$ in the interval $[0,1]$ affected by a gaussian noise $0.1*\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "> - you know that $f$ is in the form $f(x)=\\sin(kx)$ with $k\\in \\mathbb{R}$; the value of $k$ is not given\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1710844037504,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "3drUw3Pc8RWr",
    "outputId": "8317e6be-3b18-4700-97ad-2c0954a1b057"
   },
   "outputs": [],
   "source": [
    "# @title Scenario plots\n",
    "\n",
    "xA = np.random.rand(20)\n",
    "tA = np.sin(2 * np.pi * xA) + 0.1 * np.random.randn(20)\n",
    "\n",
    "xB = np.random.rand(20)\n",
    "tB = np.sin(2 * np.pi * xB) + 0.5 * np.random.randn(20)\n",
    "\n",
    "xC = np.random.rand(40)\n",
    "tC = np.sin(2 * np.pi * xC) + 0.1 * np.random.randn(40)\n",
    "\n",
    "xD = np.random.rand(100) * 0.3\n",
    "tD = np.sin(2 * np.pi * xD) + 0.1 * np.random.randn(100)\n",
    "\n",
    "xE = np.random.rand(10)\n",
    "tE = np.sin(2 * np.pi * xE) + 0.1 * np.random.randn(10)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=5,\n",
    "    subplot_titles=(\"Scenario A\", \"Scenario B\", \"Scenario C\", \"Scenario D\", \"Scenario E\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xA, y=tA, mode='markers', marker=dict(color=\"mediumpurple\")),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xB, y=tB, mode='markers', marker=dict(color=\"mediumpurple\")),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xC, y=tC, mode='markers', marker=dict(color=\"mediumpurple\")),\n",
    "              row=1, col=3)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xD, y=tD, mode='markers', marker=dict(color=\"mediumpurple\")),\n",
    "              row=1, col=4)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xE, y=tE, mode='markers', marker=dict(color=\"crimson\")),\n",
    "              row=1, col=5)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"\", range=[-1.7, 1.7])\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQDNfE2ADPUG"
   },
   "source": [
    "If you want to learn more, [here](http://bit.ly/3BLUE1BROWN2TWY9iI) you can find a nice discussion about an analogous exercise:\n",
    "> Three Amazon resellers offer a book at essentially the same price. These are their ratings:\n",
    ">\n",
    "> - 100% positive out of 10 reviews\n",
    "> - 96% positive out of 50 reviews\n",
    "> - 93% positive out of 200 reviews\n",
    ">\n",
    "> Which rating is better?\n",
    "\n",
    "> **EXERCISE (optional)**: You probably realized that in the previous exercise you wanted to be in the scenario with the largest amount of relevant information about the phenomenon $\\mathcal{P}$. Now a tricky question: the dataset of which scenario, A or B, contains more *absolute* information? (i.e. Shannon information)\n",
    ">\n",
    ">Answer [here](http://bit.ly/VeritasiumRandom3b0rPBe), or more directly [here](https://en.wikipedia.org/wiki/Quantities_of_information#Self-information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTDKBC5GL7MI"
   },
   "source": [
    "### Being linear in the parameters does not mean linear in the input\n",
    "\n",
    "In this tutorial we will restrict our search of $f$ among parametrized functions $f_\\theta$ that depend *linearly* on their finite set of parameters $\\theta = \\{a,b,c,...\\}$.\n",
    "\n",
    "Possible choices of $f$ are\n",
    "- $f_1(x) = ax + b$\n",
    "- $f_2(x) = ax^2 + bx + c$\n",
    "- $f_3(x) = a \\sin(2 \\pi x) + bx + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1710844037504,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "1C9ECfD-Q65g",
    "outputId": "dee58c09-d262-4091-dac8-342b4801d0ab"
   },
   "outputs": [],
   "source": [
    "# @title Some random plots of these linear models\n",
    "\n",
    "\n",
    "x_funcs = np.arange(0., 1.1, 2./51)\n",
    "theta_funcs = np.random.randn(10, 3)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"f1\", \"f2\", \"f3\"))\n",
    "\n",
    "for a, b in theta_funcs[:,:2]:\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=a*x_funcs + b, mode='lines'),\n",
    "                row=1, col=1)\n",
    "\n",
    "for a, b, c in theta_funcs:\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=a*x_funcs**2 + b*x_funcs + c, mode='lines'),\n",
    "                row=1, col=2)\n",
    "\n",
    "for a, b, c in theta_funcs:\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=a*np.sin(2 * np.pi * x_funcs) + b*x_funcs + c, mode='lines'),\n",
    "                row=1, col=3)\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")),\n",
    "                row=1, col=i + 1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\", range = [0,1])\n",
    "fig.update_yaxes(title_text=\"t\", range = [-2,2])\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CENyGHlSp_FX"
   },
   "source": [
    "### Loss landscapes\n",
    "\n",
    "Let's focus now on the simple $f(x) = ax + b$.\n",
    "Given our dataset $(\\mathbf{x}, \\mathbf{t})$ we will choose the parameters $a$ and $b$ that minimize a global error $E$, also known as *loss*.\n",
    "\n",
    "The standard choice for $E$ in a regression problem is the Mean Squared Error, defined as:\n",
    "\n",
    "$$E(\\theta) = \\frac{1}{N} \\sum_{i=1}^N(f_\\theta(x_i) - t_i)^2$$\n",
    "\n",
    "Notice that apart from normalization factors, the MSE corresponds to the squared $L_2$ norm of the residual vector, namely $E=\\|f_\\theta(\\mathbf{x}) - \\mathbf{t}\\|_2^2$ (here $f_\\theta$ operates element-wise).\n",
    "\n",
    "Let's now compute the MSE **loss landscape** for our dataset, so that we can visualize it. The landscape is simply defined as the value of the loss function computed over a subset of the parameter domain.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXuXlT5Sx-UE"
   },
   "outputs": [],
   "source": [
    "resolution = 51\n",
    "\n",
    "mse_losses = np.empty((resolution, resolution))\n",
    "a_range = np.linspace(-5, 5, resolution)\n",
    "b_range = np.linspace(-5, 5, resolution)\n",
    "\n",
    "for i, a in enumerate(a_range):\n",
    "    for j, b in enumerate(b_range):\n",
    "        y_pred = a * x + b\n",
    "        residuals = t - y_pred\n",
    "        loss = np.einsum('i,i->', residuals, residuals)\n",
    "        mse_losses[i][j] = loss\n",
    "\n",
    "# let's ignore the division by N for this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ63eJt6Uj5"
   },
   "source": [
    ">**EXERCISE** Vectorize the piece of code above. i.e. the two `for` cycles should disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "uAiwop63Qc1Q"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1710844037505,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "J14M4HzfaTrb",
    "outputId": "bd0533e0-9146-44d3-e295-9f53225f8c2d"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution\n",
    "\n",
    "Y_pred = x[None, None, :] * a_range[:, None, None] + b_range[None, :, None] # let broadcasting do the job!\n",
    "R = t[None, None, :] - Y_pred\n",
    "L = np.sum(R**2, axis=2)\n",
    "np.allclose(L, mse_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1710844037505,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "6B6SVoGH3kvN",
    "outputId": "183f7dd7-e833-4023-fb05-f0e025c98bde"
   },
   "outputs": [],
   "source": [
    "# @title MSE loss landscape plot\n",
    "fig = go.Figure(data=[go.Surface(z=mse_losses, x=a_range, y=b_range)])\n",
    "fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                  highlightcolor=\"limegreen\", project_z=True))\n",
    "fig.update_layout(\n",
    "    title=\"MSE loss landscape\",\n",
    "    scene = dict(\n",
    "                xaxis_title='a',\n",
    "                yaxis_title='b',\n",
    "                zaxis_title='loss')\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZy5qkSz6_2S"
   },
   "source": [
    "Look at the [convexity](https://en.wikipedia.org/wiki/Convex_function) of the MSE loss. Can you do the linear regression by sight, just hovering the mouse over it?\n",
    "\n",
    ">**EXERCISE** Visualize the **loss landscape** of other loss choices, such as the $L_1$ norm of the residual vector ($\\|f_\\theta(\\mathbf{x}) - \\mathbf{x}\\|_1)$ (absolute error), or the $L_{50}$ norm. Are these still convex?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhhE2MtUi3g_"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1710844037506,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "IrlLWxcm0tb9",
    "outputId": "0dab0cef-215e-4dd1-9800-0a39618bef28"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution (see how we did the slider!) { run: \"auto\" }\n",
    "\n",
    "p = 2  #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "\n",
    "predictions = a_range[:, None, None] * x[None, None, :]  + b_range[None, :, None]\n",
    "residuals = np.abs(t[None, None, :] - predictions) ** p\n",
    "losses = (residuals.sum(-1) ** (1/p)) / predictions.shape[-1]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=losses, x=a_range, y=b_range)])\n",
    "fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                  highlightcolor=\"limegreen\", project_z=True))\n",
    "fig.update_layout(\n",
    "    title=f\"L{p} landscape\",\n",
    "    scene = dict(\n",
    "                xaxis_title='a',\n",
    "                yaxis_title='b',\n",
    "                zaxis_title='loss')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXq55ce581os"
   },
   "source": [
    "### \"Tear down this loss[!](https://youtu.be/IguMXrgfrg8)\"\n",
    "\n",
    "Now we will explicitily compute the parameters $\\theta$ for several polynomial fits, defined in matrix notation as:\n",
    "\n",
    "![polyfit matrix notation](https://drive.google.com/uc?export=view&id=1e4AsSC00ivIXW4c35VNLladTcCTY4dpL)\n",
    "\n",
    "As seen in the lecture, we can analytically find the parameters $\\theta$ that minimize the MSE loss by solving a linear system:\n",
    "\n",
    "$$\\theta = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "Let's do it for a degree-3 polynomial fit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1Oq4K-CbWqa"
   },
   "outputs": [],
   "source": [
    "# Poly3 fit\n",
    "\n",
    "X = np.ones((t.shape[0], 4), dtype=np.float64)  # double precision is needed for this calculation, especially with high degree polyfits\n",
    "X[:,0] = x ** 3\n",
    "X[:,1] = x ** 2\n",
    "X[:,2] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv9COWdXnovB"
   },
   "source": [
    ">**EXERCISE**: complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4v9Puu-9qosN"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here (look at the formula above!)\n",
    "theta = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ck8sPJ96gd9P"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution (without einsum)\n",
    "\n",
    "theta = np.linalg.inv(X.transpose() @ X) @ X.transpose() @ t\n",
    "# theta = np.linalg.pinv(X) @ t  # you can also directly use numpy's pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4di4c5yUqkdE"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution (with einsum)\n",
    "\n",
    "temp_inverse = np.linalg.inv(np.einsum('ij,jk->ik', X.transpose(), X))  # or np.einsum('ji,jk->ik', X, X)\n",
    "theta = np.einsum('ij,jk,k->i', temp_inverse, X.transpose(), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x97OTApfhMo_"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution (numerically stable)\n",
    "\n",
    "# The equation for theta requires us to solve a linear system.\n",
    "# Instead of explicitly computing a matrix inverse and then multiplying, a more\n",
    "# efficient and stable way is to use np.linalg.solve(), which is designed to\n",
    "# handle this task more appropriately.\n",
    "\n",
    "theta = np.linalg.solve(X.transpose() @ X, X.transpose() @ t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1710844038220,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "tOJse7C9bojf",
    "outputId": "a8bb36ff-c89c-4aad-cad0-1237067124ba"
   },
   "outputs": [],
   "source": [
    "#@title Plotting our degree-3 polynomial fit\n",
    "\n",
    "# NOTE: if you get an error saying that x_funcs is undefined, execute the code\n",
    "# cell \"Some random plots of these linear models\", earlier in this notebook.\n",
    "\n",
    "a, b, c, d = theta  # unpack the ndarray\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=t, name='dataset', mode='markers', marker=dict(color=\"mediumpurple\")))\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), name='ground truth', mode='lines', line=dict(color=\"lightgreen\")))\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=a*x_funcs**3 + b*x_funcs**2 + c*x_funcs + d, name='poly3 fit', mode='lines', line=dict(color=\"lightsalmon\")))\n",
    "fig.update_xaxes(title_text=\"x\", range = [0,1])\n",
    "fig.update_yaxes(title_text=\"t\", range = [-1.5,1.5])\n",
    "fig.update_layout(width=600, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV6sseHGnDCZ"
   },
   "source": [
    "Let's now compare several polynomial fits with different degrees.\n",
    "\n",
    "Play a bit with the degrees while taking a look at the coefficients $\\theta$.\n",
    "\n",
    "Which one is the best in *your judgement*?\n",
    "Which one is the best according to the MSE loss?\n",
    "\n",
    "An evergreen reminder that here fits well:\n",
    "> **be skeptical!** Especially about the numbers that should tell us if the results we get are good or bad. Are these really evaluating what should be evaluated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1710844038221,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "uFyk0X9cfcis",
    "outputId": "63b348e3-ad80-4121-8867-2c743a24c153"
   },
   "outputs": [],
   "source": [
    "# @title Polynomial fits (press play at least once to trigger the self-update) { run: \"auto\" }\n",
    "\n",
    "degree = {}\n",
    "deg_a = 1  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_b = 4  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_c = 7 #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "\n",
    "degree[0], degree[1], degree[2] = deg_a, deg_b, deg_c\n",
    "\n",
    "max_degree = max(degree.values())\n",
    "X_full = np.ones((t.shape[0], max_degree + 1), dtype=np.float64)  # double precision is needed for this calculation!\n",
    "for i in range(max_degree):\n",
    "    X_full[:,i] =  x ** (max_degree - i)\n",
    "\n",
    "theta_collection = {}\n",
    "\n",
    "for i in range(3):\n",
    "    X = X_full[:,-(degree[i] + 1):]\n",
    "    temp_inverse = np.linalg.inv(np.einsum('ij,jk->ik', X.transpose(), X))\n",
    "    theta_collection[i] = np.einsum('ij,jk,k->i', temp_inverse, X.transpose(), t)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(f\"poly{degree[0]} fit\", f\"poly{degree[1]} fit\", f\"poly{degree[2]} fit\"))\n",
    "\n",
    "x_funcs = np.arange(0,1,1./100)\n",
    "y_funcs = {}\n",
    "for i in range(3):\n",
    "    theta_funcs = theta_collection[i]\n",
    "    X_funcs = np.ones((x_funcs.shape[0], degree[i] + 1), dtype=np.float64)\n",
    "    for j in range(degree[i]):\n",
    "        X_funcs[:,j] =  x_funcs ** (degree[i] - j)\n",
    "    y_funcs[i] = np.einsum('ij,j->i', X_funcs, theta_funcs)\n",
    "    coefficients_string = '<br><br><br>' + '<br>'.join([c + f': {np.round(theta_funcs[i], decimals=2)}' for i, c in enumerate('abcdefghijklmnopqrstuvwxyz'[:len(theta_funcs)])])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', line=dict(color=\"lightgreen\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=y_funcs[i], mode='lines', line=dict(color=\"lightsalmon\")), row=1, col=i + 1)\n",
    "    fig.update_xaxes(title_text=\"x\" + coefficients_string, range = [0,1], row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"t\", range = [-5,5])\n",
    "fig.update_layout(height=700, width=1300, title_text=\"Polynomial fits and their coefficients\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KPPrymvhAmb"
   },
   "source": [
    "Nevertheless you should find *evidence* to support your judgement. Our intuition right now is that \"High-degree polynomial fits lead to a high generalization error for this dataset\", how can we test it?\n",
    "As you have learnt in the lecture, a useful study you can perform consists in evaluating the MSE loss on a *test set*, i.e., on samples not used for training.\n",
    "\n",
    "Such as the ones that are going to magically appear in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPi-ZFkUliUA"
   },
   "outputs": [],
   "source": [
    "x_test = np.random.rand(10)\n",
    "t_test = np.sin(2 * np.pi * x_test) + 0.1 * np.random.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJu_1xXUloK0"
   },
   "source": [
    "> **EXERCISE**: Do a nice plot entitled \"MSE loss **vs** degree of the polynomial fit\" that shows the trends of the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuQphUAXhh_w"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2GV2rQ4mpNI"
   },
   "source": [
    "**NOTE**\n",
    "\n",
    "Typically, when you are designing a learning model, you would get all the available data at the beginning. Then, you split this data into training, validation, and test sets.\n",
    "\n",
    "To recap:\n",
    "\n",
    "* **Training set**: the data we use to estimate our model's parameters.\n",
    "* **Validation set**: new data we use to evaluate our model, and possibly tune its hyperparameters.\n",
    "* **Test set**: new data we use to evaluate the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVl2SP8J92tr"
   },
   "source": [
    "### Data leakage\n",
    "\n",
    "> ⚠️ Using the test set to tune your model's design and hyperparameters is not just cheating: it is *conceptually wrong*.\n",
    ">\n",
    "> In the real world, you almost never have access to the test set. For example, ChatGPT has never seen the crazy prompts we give it, but it still works well. Our input forms ChatGPT's test set. We can conclude that ChatGPT generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z1iX6ltDTx8"
   },
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENt79TByFzyD"
   },
   "source": [
    "Still, it is not always desirable to work with simple models with few parameters.\n",
    "\n",
    "Think about the true link between the input variable $x$ and the target variable $t$, it is through a $\\sin$ function.\n",
    "And a $\\sin$ function written in a polynomial basis (i.e. [its Taylor series](https://www.wolframalpha.com/input?i=taylor+series+sin+x)) contains terms of *every* odd degree, i.e. infinite terms. Our approximation of the ground truth function should benefit from high order terms, since they are *needed* to express a $\\sin$ function.\n",
    "\n",
    "We want to have many parameters, and we want them to do the right thing: to not overfit.\n",
    "This is why we need **regularizers**.\n",
    "\n",
    "Let's start with the simplest and most universal regularizer, *more data*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1710844038222,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "qcikXP28Ntfo",
    "outputId": "fc099809-07e2-4c2a-ee37-fb5e8565d370"
   },
   "outputs": [],
   "source": [
    "# @title Regularized polynomial fits: more data (again: play at least once) { run: \"auto\" }\n",
    "\n",
    "training_dataset_size = 75  #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "temp_x, temp_t = x, t\n",
    "x = np.random.rand(training_dataset_size)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(training_dataset_size)\n",
    "\n",
    "deg_1 = 2  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_2 = 6  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_3 = 13  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "\n",
    "degree = {}\n",
    "degree[0], degree[1], degree[2] = deg_1, deg_2, deg_3\n",
    "\n",
    "max_degree = max(degree.values())\n",
    "X_full = np.ones((t.shape[0], max_degree + 1), dtype=np.float64)  # double precision is needed for this calculation!\n",
    "for i in range(max_degree):\n",
    "    X_full[:,i] =  x ** (max_degree - i)\n",
    "\n",
    "theta_collection = {}\n",
    "\n",
    "for i in range(3):\n",
    "    X = X_full[:,-(degree[i] + 1):]\n",
    "    temp_inverse = np.linalg.inv(np.einsum('ij,jk->ik', X.transpose(), X))\n",
    "    theta_collection[i] = np.einsum('ij,jk,k->i', temp_inverse, X.transpose(), t)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(f\"poly{degree[0]} fit\", f\"poly{degree[1]} fit\", f\"poly{degree[2]} fit\"))\n",
    "\n",
    "x_funcs = np.arange(0,1,1./100)\n",
    "y_funcs = {}\n",
    "for i in range(3):\n",
    "    theta_funcs = theta_collection[i]\n",
    "    X_funcs = np.ones((x_funcs.shape[0], degree[i] + 1), dtype=np.float64)\n",
    "    for j in range(degree[i]):\n",
    "        X_funcs[:,j] =  x_funcs ** (degree[i] - j)\n",
    "    y_funcs[i] = np.einsum('ij,j->i', X_funcs, theta_funcs)\n",
    "    coefficients_string = '<br><br><br>' + '<br>'.join([c + f': {np.round(theta_funcs[i], decimals=2)}' for i, c in enumerate('abcdefghijklmnopqrstuvwxyz'[:len(theta_funcs)])])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', line=dict(color=\"lightgreen\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=y_funcs[i], mode='lines', line=dict(color=\"lightsalmon\")), row=1, col=i + 1)\n",
    "    fig.update_xaxes(title_text=\"x\" + coefficients_string, range = [0,1], row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"t\", range = [-5,5])\n",
    "fig.update_layout(height=700, width=1300, title_text=\"Polynomial fits and their coefficients\")\n",
    "fig.show()\n",
    "\n",
    "x, t = temp_x, temp_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdW9hYYgPAey"
   },
   "source": [
    "Unfortunately, we do not always have access to more data. Usually our goal is precisely to learn a general rule from a *few* samples, take for instance the [ARC challenge](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview).\n",
    "\n",
    "Indeed intelligence seems more related to the (small) ratio between the amount of available data and the prediction performance, rather than just to the prediction performance.\n",
    "\n",
    "Have you noticed the huge values of $\\theta$ in the case of overfitting? Aren't the coefficients of the high-order terms in the Taylor expansion of $\\sin (x)$ supposed to be smaller and smaller?\n",
    "What if we try to discourage this overfitting behavior by adding to the loss a penalization term proportional to the norm of $\\theta$?\n",
    "\n",
    "You have already seen (or will soon see) this very common regularization technique in the theoretical lectures:\n",
    "$$E'(\\theta) = \\lambda \\|\\theta\\|^2 + \\frac{1}{N} \\sum_{i=1}^N(f_\\theta(x_i) - t_i)^2$$\n",
    "\n",
    "> 📖 **EXERCISE**: Obtain the vectorized closed-form solution for $\\theta$ that minimizes $E'$. (The one for $E$ was $\\theta = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$)\n",
    ">\n",
    "> **Hint**: This is the first step:\n",
    "> $$ E'(\\theta) = (\\mathbf{y} - \\mathbf{X}\\theta)^\\top(\\mathbf{y} - \\mathbf{X}\\theta) + \\lambda\\theta^\\top\\theta$$\n",
    ">\n",
    "> Solve the exercise using pen and paper, the good old way.\n",
    "\n",
    "When you're done, let's play with the $\\lambda$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1710844038223,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "7TO701p_BoFJ",
    "outputId": "4e462f37-3554-4f7d-8e66-8bbb41d8ac77"
   },
   "outputs": [],
   "source": [
    "# @title Regularized polynomial fits: weight decay { run: \"auto\" }\n",
    "\n",
    "deg_1 = 2  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_2 = 6  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "deg_3 = 15  #@param {type:\"slider\", min:0, max:15, step:1}\n",
    "lam =   0.0001#@param {type:\"number\"}\n",
    "\n",
    "degree = {}\n",
    "degree[0], degree[1], degree[2] = deg_1, deg_2, deg_3\n",
    "\n",
    "max_degree = max(degree.values())\n",
    "X_full = np.ones((t.shape[0], max_degree + 1), dtype=np.float64)  # double precision is needed for this calculation!\n",
    "for i in range(max_degree):\n",
    "    X_full[:,i] =  x ** (max_degree - i)\n",
    "\n",
    "theta_collection = {}\n",
    "\n",
    "for i in range(3):\n",
    "    X = X_full[:,-(degree[i] + 1):]\n",
    "    temp_inverse = np.linalg.inv(np.einsum('ij,jk->ik', X.transpose(), X) + lam * np.eye(degree[i] + 1))\n",
    "    theta_collection[i] = np.einsum('ij,jk,k->i', temp_inverse, X.transpose(), t)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(f\"poly{degree[0]} fit\", f\"poly{degree[1]} fit\", f\"poly{degree[2]} fit\"))\n",
    "\n",
    "x_funcs = np.arange(0,1,1./100)\n",
    "y_funcs = {}\n",
    "for i in range(3):\n",
    "    theta_funcs = theta_collection[i]\n",
    "    X_funcs = np.ones((x_funcs.shape[0], degree[i] + 1), dtype=np.float64)\n",
    "    for j in range(degree[i]):\n",
    "        X_funcs[:,j] =  x_funcs ** (degree[i] - j)\n",
    "    y_funcs[i] = np.einsum('ij,j->i', X_funcs, theta_funcs)\n",
    "    coefficients_string = '<br><br><br>' + '<br>'.join([c + f': {np.round(theta_funcs[i], decimals=2)}' for i, c in enumerate('abcdefghijklmnopqrstuvwxyz'[:len(theta_funcs)])])\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', line=dict(color=\"lightgreen\")), row=1, col=i + 1)\n",
    "    fig.add_trace(go.Scatter(x=x_funcs, y=y_funcs[i], mode='lines', line=dict(color=\"lightsalmon\")), row=1, col=i + 1)\n",
    "    fig.update_xaxes(title_text=\"x\" + coefficients_string, range = [0,1], row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"t\", range = [-5,5])\n",
    "fig.update_layout(height=700, width=1300, title_text=\"Polynomial fits and their coefficients\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vQ8ittKYZjQ"
   },
   "source": [
    "Not so easy to tune, isn't it?\n",
    "\n",
    "Look at the magnitude of those parameters though, not spiking up anymore!\n",
    "\n",
    "It's a good moment to take a break ☕🍩, a new library awaits us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMHZ-7-okoRY"
   },
   "source": [
    "## scikit-learn\n",
    "\n",
    "Scikit-learn (also known as \"sklearn\" for brevity) is a ML library for python. It implements several ML algorithms, including the ones we have been studying so far, making them especially easy to use and hiding all the pain. It's very convenient!\n",
    "\n",
    "Importantly, scikit-learn **integrates well with matplotlib, plotly, NumPy**, and many other libraries. This makes it quite natural to include it in our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fRL5YVWmmVU"
   },
   "source": [
    "**Pro tip:** As usual, bookmark the [docs](https://scikit-learn.org/stable/user_guide.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8OPP_6ktQjh"
   },
   "outputs": [],
   "source": [
    "# @title Initial imports\n",
    "\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import random\n",
    "np.random.seed(42)  # sklearn uses numpy's rng\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmsdcpzqm9HB"
   },
   "source": [
    "### Linear regression\n",
    "\n",
    "We start by replicating one of our initial tests: a simple fit of a linear model to some random data:\n",
    "\n",
    "$$\\arg\\min_\\theta \\| \\mathbf{X}\\theta - \\mathbf{y}\\|_2^2$$\n",
    "\n",
    "Note that sklearn expects a 2D array $\\mathbf{X}$ to store the feature data. So if our data is one-dimensional, we must still encode it as a 2D tensor with just one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqWSr8dpnV2E"
   },
   "outputs": [],
   "source": [
    "# generate some random data\n",
    "N = 20\n",
    "x = np.random.rand(N)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(N)\n",
    "\n",
    "# fit a linear model\n",
    "reg = linear_model.LinearRegression()\n",
    "_ = reg.fit(x[:, None], t)  # we converted x to a 2d tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVWXdeceucV9"
   },
   "source": [
    "Done! Simply create a `LinearRegression` object, and call the `fit()` method over the data. Let's do some plots now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1710844040258,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "WKhGbTfluR7S",
    "outputId": "23f15f2f-a492-488d-b34e-a834e7ecac57"
   },
   "outputs": [],
   "source": [
    "# explicitly compute y=a*x+b using the learned a and b\n",
    "y_pred2 = reg.coef_ * x + reg.intercept_\n",
    "\n",
    "# ...or directly use the predict() method to do it for you!\n",
    "y_pred = reg.predict(x[:, None])\n",
    "\n",
    "plt.scatter(x, t, color=\"mediumpurple\")\n",
    "plt.plot(x, y_pred, color=\"lightsalmon\", linewidth=3)\n",
    "plt.plot(x, y_pred2, color=\"black\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfpRSqpsxqnx"
   },
   "source": [
    "### Polynomial regression\n",
    "\n",
    "Time to move on to polynomial fitting!\n",
    "\n",
    "As we have seen during the theory class, this is still a _linear regression_ problem, but uses a different data matrix. Scikit-learn makes this very explicit, providing functions to transform the data from linear to **polynomial features** of any degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1710844040258,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "Nls09C2VzPpj",
    "outputId": "ee65bac8-b1e0-4965-fb7c-4b58eb927762"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# generate some random data\n",
    "N = 20\n",
    "x = np.random.rand(N)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(N)\n",
    "\n",
    "# compute polynomial features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X = poly.fit_transform(x[:, None])  # again, it expects a 2D tensor\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a232vIp0UFu"
   },
   "source": [
    "Observe the following:\n",
    "\n",
    "- We started with $N$ points, stored as **1d features** $[x]$ in tensor `x`.\n",
    "- We obtained $N$ **4d polynomial features** $[1, x, x^2, x^3]$, stored in `X`.\n",
    "- The first dimension is always $x^0 =1$.\n",
    "\n",
    "We can now use these features to train any linear model.\n",
    "\n",
    "💡 Thinking about data _features_, as opposed to the raw representation given as input, opens up a lot of directions! For example, with deep learning we will actually _learn_ the best features that solve a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1710844040259,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "n3A5LBCB1eHP",
    "outputId": "cb4de69e-2547-485b-caa2-b9d0b3d4840f"
   },
   "outputs": [],
   "source": [
    "# linear fit with polynomial features\n",
    "reg = linear_model.LinearRegression()\n",
    "_ = reg.fit(X, t)\n",
    "\n",
    "a, b, c, d = reg.coef_\n",
    "e = reg.intercept_\n",
    "\n",
    "# plot\n",
    "x_funcs = np.arange(0., 1.1, 2./51)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=t, mode='markers', name='data', marker=dict(color=\"mediumpurple\")))\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', name='ground-truth', line=dict(color=\"lightgreen\")))\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=b*x_funcs+c*(x_funcs**2)+d*(x_funcs**3)+e, mode='lines', name='poly3 fit', line=dict(color=\"lightsalmon\")))\n",
    "fig.update_layout(width=600, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7cCE2pGB7RF"
   },
   "source": [
    "#### Fitting a trajectory\n",
    "\n",
    "Let's do something a bit more interesting.\n",
    "\n",
    "So far we have been trying to fit functions $f:\\mathbb{R}\\to\\mathbb{R}$, but we can be more general than this and consider high-dimensional mappings $f:\\mathbb{R}^m \\to \\mathbb{R}^n$. For example, a parametric curve on the plane would be modeled as a function $f:\\mathbb{R}\\to\\mathbb{R}^2$.\n",
    "\n",
    "Here's an example of what I like to call the [Zool](https://www.mobygames.com/game/5776/zool/) curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1710844041013,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "3WySEidEB7RF",
    "outputId": "beec24a5-4f70-4ff8-b4ed-53f7f63f0ef4"
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, np.pi, 100)\n",
    "p = np.array((2*np.cos(2*t), np.sin(4*t)))\n",
    "\n",
    "plt.plot(p[0, :], p[1, :], linestyle='-', color='b')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTODPPtZB7RH"
   },
   "source": [
    "> **EXERCISE**\n",
    ">\n",
    "> Fit a polynomial to the data given below, which was generated from a 3D spiral defined as:\n",
    ">\n",
    "> $$f(t) = (\\cos(t), \\sin(t), 0.1t)\\,,\\quad\\quad t \\in [0, 5\\pi]$$\n",
    ">\n",
    "> Choose a polynomial degree by yourself. What happens if it's too small? Or too large?\n",
    ">\n",
    "> Create a nice plot of your solution, showing the polynomial curve passing through the training points. You can use Plotly's `go.Scatter3d` for drawing in 3D.\n",
    ">\n",
    "> _Remember:_ The true $f(t)$ is **unknown**, so you can't use the formula above to find a solution. The only input to your training model is $N$ data points $(t_i, f(t_i))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slbtfpZ7B7RH"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "t = np.linspace(0, 5*np.pi, 40)\n",
    "p = np.array((np.cos(t), np.sin(t), 0.1*t))\n",
    "\n",
    "# ✏️ your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1710844041013,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "GA4s2SgqB7RI",
    "outputId": "2eac1d9b-2c1d-45a9-a24c-3039fe1638d3"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution\n",
    "\n",
    "\n",
    "# training data\n",
    "t = np.linspace(0, 5*np.pi, 40)\n",
    "p = np.array((np.cos(t), np.sin(t), 0.1*t))\n",
    "\n",
    "# Make sure you understand the problem precisely.\n",
    "# - You are given training data as pairs (t, (x,y,z)).\n",
    "# - You want to express each coordinate x, y, z as a *polynomial in t*.\n",
    "# - Each coordinate will get its own set of learned parameters.\n",
    "# Once you got this down, the rest is straightforward!\n",
    "\n",
    "degree = 10\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "T = poly.fit_transform(t[:, None])\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "_ = reg.fit(T, p.transpose())\n",
    "\n",
    "# do the plots\n",
    "ts = t[: ,None] ** np.arange(degree+1)\n",
    "P = np.einsum('ij, kj -> ik', ts, reg.coef_) + reg.intercept_\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Training data\", \"Polynomial fit\"), specs=[[{'type': 'scene'}, {'type': 'scene'}]])\n",
    "fig.add_trace(go.Scatter3d(x=p[0,:], y=p[1,:], z=p[2,:], mode='markers', marker=dict(opacity=1, size=5, color=t, colorscale=\"viridis\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter3d(x=p[0,:], y=p[1,:], z=p[2,:], mode='markers', marker=dict(opacity=0.5, size=5, color=t, colorscale=\"viridis\")), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=P[:,0], y=P[:,1], z=P[:,2], mode='lines', line=dict(color='red', width=3)), row=1, col=2)\n",
    "fig.update_layout(title='Trajectory fitting', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qgsk1rnB7RI"
   },
   "source": [
    "> **EXERCISE:**\n",
    ">\n",
    "> What is the total number of parameters for the polynomial regression model used above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P75VbQ70B7RJ"
   },
   "source": [
    "#### Piecewise-polynomial fitting 📖\n",
    "\n",
    "Some of the most interesting problems ML arise when you deal with **noisy, corrupted, or incomplete** data. Imagine being able to upscale a low-quality video to 1080p resolution; or removing the blur from a shaky photo; or fill up noisy parts of a badly recorded song with clean audio data. ML to the rescue!\n",
    "\n",
    "Linear regression is not really the tool of the trade to solve this kind of problems. But let's try anyway. We are going to consider a **gray-scale image**, seen as a function:\n",
    "\n",
    "$$f :\\mathbb{R}^2 \\to \\mathbb{R} $$\n",
    "\n",
    "Suppose we are given the pixel values only for some of the points. We are going to use degree-3 polynomial regression to \"fill up the holes\".\n",
    "\n",
    "> 📁 **NOTE:**\n",
    "> Make sure you have the image \"cat.png\" in your Drive folder.\n",
    ">\n",
    "> You don't really need to read and understand all the code in the next cell. We are just running regression on pixel data, and plot the predictions on the entire pixel grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "error",
     "timestamp": 1710844041856,
     "user": {
      "displayName": "Emanuele Rodolà",
      "userId": "04078797398573274489"
     },
     "user_tz": -60
    },
    "id": "PBUtQuwWB7RJ",
    "outputId": "9b3c351f-38be-4c20-c98c-04269c26d1c4"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from PIL import Image\n",
    "\n",
    "image_path = './cat.png'\n",
    "image = Image.open(image_path).convert('L')  # convert to grayscale\n",
    "image_array = np.array(image)\n",
    "\n",
    "fraction = 0.8  # Fraction of pixels to keep\n",
    "degree = 3  # Polynomial degree\n",
    "\n",
    "# Flatten the image array and create coordinates grid\n",
    "y, x = np.indices(image_array.shape)\n",
    "x_flat = x.ravel()\n",
    "y_flat = y.ravel()\n",
    "image_flat = image_array.ravel()\n",
    "\n",
    "# Randomly select a fraction of pixels\n",
    "total_pixels = x_flat.size\n",
    "num_selected = int(total_pixels * fraction)\n",
    "selected_indices = np.random.choice(total_pixels, num_selected, replace=False)\n",
    "\n",
    "# Prepare data for polynomial regression\n",
    "X_selected = np.stack((x_flat[selected_indices], y_flat[selected_indices]), axis=1)\n",
    "y_selected = image_flat[selected_indices]\n",
    "\n",
    "# Fit Ridge regression model on selected pixels\n",
    "model = make_pipeline(PolynomialFeatures(degree), linear_model.LinearRegression())\n",
    "model.fit(X_selected, y_selected)\n",
    "\n",
    "# Predict for the whole image\n",
    "X_all = np.stack((x_flat, y_flat), axis=1)\n",
    "image_recovered_flat = model.predict(X_all)\n",
    "image_recovered = image_recovered_flat.reshape(image_array.shape)\n",
    "\n",
    "# Visualize the original, recovered images, and kept pixels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axes[0].imshow(image_array, cmap='gray')\n",
    "axes[0].set_title('Original ground-truth image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show the kept pixels on a black background\n",
    "kept_pixels = np.zeros_like(image_array)\n",
    "kept_pixels.flat[selected_indices] = image_array.flat[selected_indices]\n",
    "axes[1].imshow(kept_pixels, cmap='gray')\n",
    "axes[1].set_title('Noisy pixels')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image_recovered, cmap='gray')\n",
    "axes[2].set_title('Recovered image')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvZ4vpzpB7RK"
   },
   "source": [
    "Not working too well, right? The recovered image is super blurry and flat. Try using higher-order polynomials to at least overfit the input pixel data, and see how far you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AVvCy-JB7RL"
   },
   "source": [
    "> ⚠️ **On the number of parameters**\n",
    ">\n",
    "> Careful here!\n",
    ">\n",
    "> We are in the setting where the function to learn is $f: \\mathbb{R}^2 \\to \\mathbb{R}$. Therefore, for a degree-2 polynomial regression problem, we are trying to fit a polynomial such that:\n",
    ">\n",
    "> $f(x, y) = ax^2 + by^2 + cxy + dx + ey + f$\n",
    ">\n",
    "> for all points $(x,y) \\in \\mathbb{R}^2$. Notice the **cross-terms** $xy$. Importantly, the parameters $\\{a,b,c,d\\}$ are going to be **the same for all pixels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ClIY3ZeB7RL"
   },
   "source": [
    "> **EXERCISE:**\n",
    ">\n",
    "> What is the total number of parameters for the polynomial regression model used above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9wBJyjqB7RL"
   },
   "source": [
    "Let's try to improve upon our horrible result. Instead of fitting a polynomial to all the pixels at once, let's split the given pixels into $M$ **patches**. Then, let's solve $M$ distinct polynomial regression problems, one per patch, and then stitch together the results to recompose a full image. Ready?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66T27qqlB7RM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load image\n",
    "image_path = './cat.png'\n",
    "image = Image.open(image_path).convert('L')\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Parameters\n",
    "patch_size = 5  #  patch size in pixels\n",
    "degree = 3\n",
    "fractions = np.linspace(0.98, 0.04, 10)  # Fraction of pixels to keep in each patch\n",
    "\n",
    "fig, axes = plt.subplots(len(fractions), 2, figsize=(12, 4*len(fractions)))\n",
    "\n",
    "for idx, fraction in enumerate(fractions):\n",
    "\n",
    "    # Initialize the arrays for the reconstructed image and kept pixels\n",
    "    reconstructed = np.zeros_like(image_array)\n",
    "    kept_pixels = np.zeros_like(image_array)\n",
    "\n",
    "    # Function to process each patch with selected fraction of pixels\n",
    "    def process_patch(x_patch, y_patch, patch, fraction):\n",
    "        # Flatten patch coordinates and values\n",
    "        X_patch = np.array([x_patch.flatten(), y_patch.flatten()]).T\n",
    "        y_patch = patch.flatten()\n",
    "\n",
    "        # Randomly select a fraction of pixels\n",
    "        selected = np.random.choice(X_patch.shape[0], int(X_patch.shape[0] * fraction), replace=False)\n",
    "        X_selected = X_patch[selected]\n",
    "        y_selected = y_patch[selected]\n",
    "\n",
    "        # Fit linear regression model on selected pixels\n",
    "        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "        model.fit(X_selected, y_selected)\n",
    "\n",
    "        # Predict for the whole patch and mark kept pixels\n",
    "        predictions = model.predict(X_patch).reshape(patch.shape)\n",
    "        kept_patch = np.zeros_like(patch)\n",
    "        kept_patch.flat[selected] = y_patch.flat[selected]\n",
    "        return predictions, kept_patch\n",
    "\n",
    "    # Process the image in patches\n",
    "    num_rows, num_cols = image_array.shape\n",
    "    for i in range(0, num_rows, patch_size):\n",
    "        for j in range(0, num_cols, patch_size):\n",
    "            row_end = min(i + patch_size, num_rows)\n",
    "            col_end = min(j + patch_size, num_cols)\n",
    "            patch = image_array[i:row_end, j:col_end]\n",
    "            x_patch, y_patch = np.meshgrid(np.arange(j, col_end), np.arange(i, row_end))\n",
    "\n",
    "            # Reconstruct the patch and get kept pixels\n",
    "            reconstructed_patch, kept_patch = process_patch(x_patch, y_patch, patch, fraction)\n",
    "            reconstructed[i:row_end, j:col_end] = reconstructed_patch\n",
    "            kept_pixels[i:row_end, j:col_end] = kept_patch\n",
    "\n",
    "    axes[idx][0].imshow(kept_pixels, cmap='gray')\n",
    "    axes[idx][0].set_title(f'Input data ({fraction*100:.2f}%)')\n",
    "    axes[idx][0].axis('off')\n",
    "\n",
    "    axes[idx][1].imshow(reconstructed, cmap='gray')\n",
    "    axes[idx][1].set_title('Reconstructed Image')\n",
    "    axes[idx][1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gJU3ltfB7RM"
   },
   "source": [
    "A few observations are in order:\n",
    "\n",
    "- Even for very extreme cases (**4%** of all the input pixels!) we get reasonable results.\n",
    "- The **smooth** portions of the image (e.g. the background) are always reconstructed well.\n",
    "- The portions with lots of detail (e.g. the cat fur and whiskers) are the most difficult to recover.\n",
    "\n",
    "We are essentially modeling our image as a **piecewise-polynomial function**. This allows us to accurately model more complex functions, at the cost of more parameters and more processing. As we shall see throughout the course, composing together simple functions is a powerful idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8m6o8TyB7RN"
   },
   "source": [
    "> **EXERCISE:**\n",
    ">\n",
    "> What is the total number of parameters for the piecewise-polynomial regression model used above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51aX21ec6-12"
   },
   "source": [
    "### Ridge regression (linear + Tikhonov)\n",
    "\n",
    "Let us now consider the $L_2$ penalty we used earlier to **penalize large parameter values**. We are now looking at the regularized problem:\n",
    "\n",
    "$$\\arg\\min_\\theta \\| \\mathbf{X}\\theta - \\mathbf{y}\\|_2^2 + \\alpha \\|\\theta\\|_2^2$$\n",
    "\n",
    "where $\\alpha>0$ controls the trade-off between _data fidelity_ and _regularization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4WERgIU8YT0"
   },
   "source": [
    "> **EXERCISE**\n",
    ">\n",
    "> If you haven't already solved this in the \"Regularization\" section earlier in the notebook:\n",
    ">\n",
    "> 1. Derive the normal equation for ridge regression.\n",
    "> 2. Show that it is equivalent to standard linear regression, plus a _diagonal_ update.\n",
    ">\n",
    ">_Hint:_ start from rewriting the loss as $(\\mathbf{X}\\theta - \\mathbf{y})^\\top(\\mathbf{X}\\theta - \\mathbf{y}) + \\alpha \\theta^\\top\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrKcoGRY_NSO"
   },
   "source": [
    "We know how to solve this problem: write the normal equations and solve the system. Or we can use sklearn's `Ridge` class to do it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moCC2Elw_aJT"
   },
   "outputs": [],
   "source": [
    "# generate some random data\n",
    "N = 25\n",
    "x = np.random.rand(N)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(N)\n",
    "\n",
    "# compute polynomial features of high degree\n",
    "poly = PolynomialFeatures(degree=17)\n",
    "X = poly.fit_transform(x[:, None])\n",
    "\n",
    "# standard linear regression\n",
    "reg_linear = linear_model.LinearRegression()\n",
    "_ = reg_linear.fit(X, t)\n",
    "\n",
    "# ridge regression\n",
    "reg_ridge = linear_model.Ridge(alpha=0.001)\n",
    "_ = reg_ridge.fit(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csax63irGEfQ"
   },
   "source": [
    "I hope you appreciate how clean the code above is! Let's see how it's actually performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZZgxZ5sQFixN"
   },
   "outputs": [],
   "source": [
    "# @title Plot linear vs. ridge regression\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(f\"poly{X.shape[1]-1} fit\", f\"regularized poly{X.shape[1]-1} fit\"))\n",
    "\n",
    "X_funcs = x_funcs[:, None] ** np.arange(X.shape[1])\n",
    "Y_linear = np.einsum('ij, j -> i', X_funcs, reg_linear.coef_) + reg_linear.intercept_\n",
    "Y_ridge = np.einsum('ij, j -> i', X_funcs, reg_ridge.coef_) + reg_ridge.intercept_\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=t, mode='markers', marker=dict(color=\"mediumpurple\")), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', line=dict(color=\"lightgreen\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=np.sin(2*np.pi*x_funcs), mode='lines', line=dict(color=\"lightgreen\")), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=Y_linear, mode='lines', line=dict(color=\"lightsalmon\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_funcs, y=Y_ridge, mode='lines', line=dict(color=\"lightsalmon\")), row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"t\", range = [-2,2])\n",
    "fig.update_xaxes(title_text=\"x\", range = [0,1])\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBm33uCkG9cP"
   },
   "source": [
    "> **EXERCISE**\n",
    ">\n",
    "> Using the data from the previous exercise, create a plot showing how the values of the learned _weights_ change with increasing $\\alpha$.\n",
    ">\n",
    "> Draw one curve per weight, all in the same plot. For example, for a degree-17 polynomial, the plot should contain 18 curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gARQT3B1OcmW"
   },
   "outputs": [],
   "source": [
    "# ✏️ your solution here\n",
    "\n",
    "# generate some random data\n",
    "N = 25\n",
    "x = np.random.rand(N)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(N)\n",
    "\n",
    "# use these values for alpha\n",
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)  # spaced evenly on a log scale from 1e-10 to 1e-2\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-sl-BcRpK5dI"
   },
   "outputs": [],
   "source": [
    "# @title 👀 Solution\n",
    "\n",
    "# generate some random data\n",
    "N = 25\n",
    "x = np.random.rand(N)\n",
    "t = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(N)\n",
    "\n",
    "# use these values for alpha\n",
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -2, n_alphas)  # spaced evenly on a log scale from 1e-10 to 1e-2\n",
    "\n",
    "# compute polynomial features of high degree\n",
    "poly = PolynomialFeatures(degree=17)\n",
    "X = poly.fit_transform(x[:, None])\n",
    "\n",
    "# this will store the weights for all alphas (not storing the bias)\n",
    "weights = np.zeros((len(alphas), X.shape[1]), dtype=np.float32)\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "  reg_ridge = linear_model.Ridge(alpha=alpha)\n",
    "  _ = reg_ridge.fit(X, t)\n",
    "  weights[i] = reg_ridge.coef_\n",
    "\n",
    "print(weights.shape)\n",
    "\n",
    "# plot\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, weights)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"weights\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrHbGNbeB7RS"
   },
   "source": [
    "Observe how the parameters get _smaller_ for large $\\alpha$. This is expected, because giving more weight to the regularizer implies a stronger penalty on the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy_I1l52B7RS"
   },
   "source": [
    "> **EXERCISE**\n",
    ">\n",
    "> Using the same data as the previous exercise, do standard linear regression instead of ridge regression.\n",
    ">\n",
    "> What's the difference in scale between the parameters of the two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WLYo3mKB7RT"
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "Finally, let's implement a simple $n$-fold cross-validation scheme. Recall the recipe from the theory class:\n",
    "\n",
    "1. Split the training data into $n$ subsets (\"folds\").\n",
    "2. Select one fold and keep it; we'll use it as a validation set.\n",
    "3. Train you model on the remaining $n-1$ folds and save the model parameters.\n",
    "4. Go back to step 2.\n",
    "5. Stop when you've trained $n$ models.\n",
    "\n",
    "This idea of training a model on all data except for one fold is also called **leave-one-out** in ML jargon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXg_9THUB7RT"
   },
   "source": [
    "> **EXERCISE**\n",
    ">\n",
    "> Using Scikit-learn, implement 5-fold cross-validation to test a model that performs degree-6 polynomial regression on data of your choice.\n",
    ">\n",
    "> To test your model on unseen data, use the `predict()` call from the `LinearRegression` class.\n",
    ">\n",
    "> Create a figure showing the predictions of the 5 models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4C5Ct9yoZKYa",
    "dHLO4Z-T_yxB",
    "GPHftSvUaIhe",
    "0PAIT1NjFUhk",
    "8htU_ACVGQY5",
    "i8kq5mSFOHul",
    "mhRdUyWpwX-v",
    "yTDKBC5GL7MI",
    "CENyGHlSp_FX",
    "FXq55ce581os",
    "5z1iX6ltDTx8",
    "Tvpx3YbdcKlK",
    "ndnx7HJxkvXH",
    "SYYP8rCj2-Ae",
    "Q4l1TIvepi6H",
    "zFEIytCKlMsQ",
    "DUhUSCRNoTQb"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
